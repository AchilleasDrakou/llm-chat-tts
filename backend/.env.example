# LLM Chat TTS API - Environment Variables
# Copy this file to .env and fill in your values

# ===================
# API Keys
# ===================
# OpenAI API key for chat functionality
OPENAI_API_KEY=your_openai_api_key_here

# ===================
# Server Configuration
# ===================
# Host for the FastAPI server
HOST=0.0.0.0
# Port for the FastAPI server
PORT=8000
# Debug mode (true/false)
DEBUG=false
# Environment (development/production)
ENVIRONMENT=development
# CORS allowed origins (comma-separated list)
CORS_ORIGINS=http://localhost:3000,http://localhost:1420

# ===================
# Logging Configuration
# ===================
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO
# Log file path
LOG_FILE=logs/app.log
# Log rotation size in MB
LOG_ROTATION_SIZE=10
# Enable JSON serialization for logs (true/false)
LOG_SERIALIZE=true

# ===================
# TTS Configuration
# ===================
# Device for TTS model (cuda, cpu, mps)
TTS_DEVICE=cuda
# Default voice
TTS_DEFAULT_VOICE=default
# Default exaggeration factor (0.0-1.0)
TTS_DEFAULT_EXAGGERATION=0.5
# Default CFG weight (0.0-1.0)
TTS_DEFAULT_CFG_WEIGHT=0.5
# Cache directory for TTS audio files
TTS_CACHE_DIR=cache
# Maximum cache size (number of items)
TTS_CACHE_SIZE=100

# ===================
# Performance Settings
# ===================
# Number of workers for Uvicorn
WORKERS=4
# Request timeout in seconds
REQUEST_TIMEOUT=60
# Maximum content length for requests (in MB)
MAX_CONTENT_LENGTH=10
